# -*- coding: utf-8 -*-
"""content-based

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qDsIQjvcA46l5YbxSFADVCUlJ1DKBiqY

# Bima Surya Nurwahid
# M 06
# m183x0325
 Universitas Amikom Yogyakarta

#Setting Kaggle
"""

!pip install -q kaggle

from google.colab import files
files.upload()

# Library 
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import os
from os.path import join

# Library Visualisasi
import matplotlib.pyplot as plt
import seaborn as sns

# Modeling library
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

"""#Import Dataset"""

!kaggle datasets download -d CooperUnion/anime-recommendations-database

!unzip '/content/anime-recommendations-database.zip'

anime = pd.read_csv('/content/anime.csv')
anime.head(5)

anime.shape

anime.columns

anime = anime.drop(columns=['anime_id'])

anime.head(20)

"""#Eksploratory Data Analysis"""

def printByInformation(dataset, option=False):
  if option:
    pd.set_option('display.max_columns',None)
    print(f'current rows:{dataset.shape[0]}')
    print(f'current col:{dataset.shape[1]}')
    print('======================DATA CLEANING=======================================')
    print(f'jumlah NaN {dataset.isnull().sum().sum()} dari NaN yg ditemukan')
    print(f'jumlah NaN tiap Column\n{dataset.isnull().sum()}')
    print(f'Name Columns: {list(dataset.columns)}')
    print(f'{dataset.info()}')
    print(f'{dataset.describe()}')
printByInformation(anime,True)

print('This is a list of genres: ', anime.genre.unique())

"""#Data Cleaning"""

null_features = anime.columns[anime.isna().any()]
anime[null_features].isna().sum()

anime.dropna(inplace=True)
anime[null_features].isna().sum()

"""#Data Preprocessing"""

plt.figure(figsize=(15,15))
sns.countplot(y=anime['type'])
plt.show()

anime.drop_duplicates(subset='name',inplace=True)

"""#Modelling"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data genre
tf.fit(anime['genre']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

# Melakukan fit lalu ditransformasikan ke dalam bentuk matrix
tfidf_matrix = tf.fit_transform(anime['genre']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=anime.name
).sample(22, axis=1).sample(10, axis=0)

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa name anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=anime['name'], columns=anime['name'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap anime
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def anime_recom(anime_name, similarity_data=cosine_sim_df, items=anime[['name', 'genre', 'episodes', 'rating']], k=5):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,anime_name].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar/tertinggi dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop name anime agar name yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(anime_name, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

anime[anime['name'] == 'Shigatsu wa Kimi no Uso']

anime_recom('Shigatsu wa Kimi no Uso')

anime[anime['name'] == 'Gintama']

anime_recom('Gintama')

"""**DONE**"""

anime[anime['name'] == 'One Piece']

anime_recom('One Piece')

